{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 쯂ullback que?\n",
        "\n",
        "Cuando trabajamos con modelos que representan una distribuci칩n de probabilidad nuestro objetivo es hacer que la distribuci칩n de los datos se acerque lo m치s posible a las probabilidades que nos da el modelo sobre esos datos. Existen muchas maneras de calcular esa diferencia, una com칰n es usar funciones de divergencia, entre ellas la divergencia de Kullback-Leibler es la m치s usada. Dadas dos distribuciones de probabilidad 洧녞 y 洧녟 se define asumiendo que sean distribuciones discretas como:\n",
        "\n",
        "$$\n",
        "KL(P|Q)=\\sum_{i=1}^{N}P(i)\\cdot \\log(\\frac{P(i)}{Q(i)})\n",
        "$$\n",
        "\n",
        "En el caso de distribuciones continuas, simplemente substituimos el sumatorio por una integral.\n",
        "\n"
      ],
      "metadata": {
        "id": "tNlYxswldlOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJTYZijMdg_D"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import make_classification\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit"
      ],
      "metadata": {
        "id": "9f7kVGAJf12-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 1"
      ],
      "metadata": {
        "id": "tv_mwq9mg7Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Todo modelo de clasificaci칩n es una distribuci칩n de probabilidad sobre un conjunto de valores discretos, por lo que podemos ajustar un modelo probabil칤stico para clasificaci칩n haciendo que las probabilidades que obtenga para una muestra se ajusten a las de los datos. Usa la funci칩n make_classification de scikit-learn para crear un conjunto de datos de clasificaci칩n de dos dimensiones y 100 ejemplos. Tendr치s dar un valor 0 al par치metro n_redundant y un valor 1 al par치metro n_clusters_per_class. Da un valor tambi칠n al par치metro random_state para que los experimentos sean reproducibles. El problema que generar치 ser치 de clasificaci칩n binaria."
      ],
      "metadata": {
        "id": "ABudX8lbfskY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation to make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n"
      ],
      "metadata": {
        "id": "1xg3L-mJgVOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = make_classification(n_samples=100, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "ThekmQiTgRZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}