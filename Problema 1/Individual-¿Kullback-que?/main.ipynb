{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IFibla/Aprenentatge-Automatic/blob/master/Problema%201/Individual-%C2%BFKullback-que%3F/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Kullback que?\n",
        "\n",
        "Cuando trabajamos con modelos que representan una distribución de probabilidad nuestro objetivo es hacer que la distribución de los datos se acerque lo más posible a las probabilidades que nos da el modelo sobre esos datos. Existen muchas maneras de calcular esa diferencia, una común es usar funciones de divergencia, entre ellas la divergencia de Kullback-Leibler es la más usada. Dadas dos distribuciones de probabilidad 𝑃 y 𝑄 se define asumiendo que sean distribuciones discretas como:\n",
        "\n",
        "$$\n",
        "KL(P|Q)=\\sum_{i=1}^{N}P(i)\\cdot \\log(\\frac{P(i)}{Q(i)}) \n",
        "$$\n",
        "\n",
        "En el caso de distribuciones continuas, simplemente substituimos el sumatorio por una integral.\n",
        "\n"
      ],
      "metadata": {
        "id": "tNlYxswldlOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJTYZijMdg_D"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad, jit\n",
        "from sklearn.datasets import make_classification, make_circles\n",
        "import jax.numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "9f7kVGAJf12-"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 1"
      ],
      "metadata": {
        "id": "tv_mwq9mg7Iu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "HJexNey3i2Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_make_classification_list(in_list):\n",
        "  out_list = []\n",
        "  for i in range(len(in_list[0])):\n",
        "    out_list.append([l[i] for l in in_list])\n",
        "  return out_list"
      ],
      "metadata": {
        "id": "i3VcSnL1NrAB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_matplotlib_histogram(X, labelX, labelY, title, rangeX=None, rangeY=None, bins=30):\n",
        "  if rangeX is None: \n",
        "    rangeX = [np.min(X), np.max(X)]\n",
        "  plt.hist(X, bins=bins, alpha=1, range=rangeX, density=True, stacked=True)  \n",
        "  plt.xlabel(labelX)\n",
        "  plt.ylabel(labelY)\n",
        "  plt.title(title)\n",
        "  if rangeY is not None:\n",
        "    plt.ylim((rangeY[0], rangeY[1]))\n",
        "  plt.legend([len(x) for x in X] if len(X) > 1 else \"X\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gn2wU130i6E-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Siendo 𝑋 una muestra de datos 𝑥1, ... , 𝑥𝑛 de valores discretos, donde podemos estimar su distribución 𝑃 a partir de su frecuencia y 𝑄 es una distribución de probabilidad sobre el mismo rango de valores discretos. Demuestra que optimizar 𝐾 𝐿(𝑃 |𝑄) es equivalente a optimizar la log verosimilitud negativa de 𝑄 sobre los datos."
      ],
      "metadata": {
        "id": "hWDNwM-oQZiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_minus_log_verosimilitud_q = lambda P: sum(1/np.log(P)) - (len(P) * np.log(len(P)))\n",
        "a_diff_minus_log_verosimilitud_q = lambda P: sum(1/np.log(P))\n",
        "\n",
        "a_optimise_kl = lambda P: (len(P) + len(P) * np.log(len(P))) * sum(P)\n",
        "a_diff_optimise_kl = lambda P: (len(P) + len(P) * np.log(len(P))) * sum(P)"
      ],
      "metadata": {
        "id": "620AnuQNQg10"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Todo modelo de clasificación es una distribución de probabilidad sobre un conjunto de valores discretos, por lo que podemos ajustar un modelo probabilístico para clasificación haciendo que las probabilidades que obtenga para una muestra se ajusten a las de los datos. Usa la función make_classification de scikit-learn para crear un conjunto de datos de clasificación de dos dimensiones y 100 ejemplos. Tendrás dar un valor 0 al parámetro n_redundant y un valor 1 al parámetro n_clusters_per_class. Da un valor también al parámetro random_state para que los experimentos sean reproducibles. El problema que generará será de clasificación binaria."
      ],
      "metadata": {
        "id": "ABudX8lbfskY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n"
      ],
      "metadata": {
        "id": "1xg3L-mJgVOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_sample = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=RANDOM_STATE)\n",
        "b_sample_prob = np.array(resize_make_classification_list(b_sample[0]))\n",
        "b_sample_label = np.array(b_sample[1])\n",
        "b_sample_prob, b_sample_label"
      ],
      "metadata": {
        "id": "ThekmQiTgRZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Podemos crear un modelo probabilístico con una función linear $f(w,x)=w\\cdot x$. Para obtener probabilidades simplemente tenemos que aplicar sobre el resultado una función que de un valor entre 0 y 1. Por ejemplo la función sigmoide 𝜎:\n",
        ">\n",
        "> $σ(x)=\\frac{1}{1+e^x}$\n",
        ">\n",
        "> A partir de la divergencia de Kullback-Leibler simplificando para problemas binarios podemos llegar a la función de pérdida de entropía cruzada binaria (binary cross entropy):\n",
        ">\n",
        "> $BCE(p(x), y )=y \\cdot \\log(p(x)) + (1-y) \\cdot \\log(1-p(x))$\n",
        ">\n",
        "> Donde 𝑝(𝑥) es la probabilidad que le asigna el modelo a un ejemplo, e 𝑦 es la etiqueta que le corresponde a los datos. Implementa un algoritmo de descenso de gradiente usando JAX con la función de entropía cruzada binaria. Explora diferentes tasas de aprendizaje. Comenta lo que observes en el comportamiento del error y los parámetros durante la optimización. Escoge un número de iteraciones y un valor para decidir el final de la optimización que te parezcan adecuados."
      ],
      "metadata": {
        "id": "CmlvJqMkQYJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. jax.grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html)"
      ],
      "metadata": {
        "id": "z_lBNg2QVmte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_sigmoid_function = lambda x: 1 / (1 + np.exp(x))\n",
        "cross_binary_entropy = lambda Px, y: np.array(y * np.log(Px) + ((1 - y) * np.log(1-Px))).mean()"
      ],
      "metadata": {
        "id": "4Y-JlCQ1dJgS"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_gradient_function = grad(cross_binary_entropy, argnums=1, allow_int=True)"
      ],
      "metadata": {
        "id": "RzBjZYtBVEM6"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_sample_prob, c_sample_label = c_sigmoid_function(b_sample_prob), b_sample_prob"
      ],
      "metadata": {
        "id": "DZDxEALGb-8F"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_gradient_function(c_sample_prob, c_sample_label)"
      ],
      "metadata": {
        "id": "kzHIt3I0ejPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Genera un conjunto de datos con la función make_circles de scikit-learn usando el valor 0,1 para el parámetro noise. Optimiza el modelo para varios parámetros iniciales diferentes del modelo. Cuenta que esta que sucediendo e intenta explicar el porqué."
      ],
      "metadata": {
        "id": "ua1BColsfU3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. sklearn.datasets.make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)"
      ],
      "metadata": {
        "id": "j3Ah5WYpfbID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_sample = make_circles(n_samples=100, noise=0.1, random_state=RANDOM_STATE)\n",
        "d_sample"
      ],
      "metadata": {
        "id": "EhwJHwIzfYO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> La función de entropía cruzada parece una función extraña para optimizar cuando lo que nos interesa es un modelo que tenga el mínimo número de ejemplos mal clasificados. En este caso se correspondería a la función de pérdida 0/1, que en el caso de probabilidades asignaría una pérdida de 0 a valores menores que 0.5 y 1 en caso contrario ¿Porqué no es una buena idea optimizar directamente esta función? Representa las dos funciones."
      ],
      "metadata": {
        "id": "3OvLeqrTgYYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_loss_function = lambda Px: 0 if Px < 0.5 else 1"
      ],
      "metadata": {
        "id": "Y8gwO9EWgeFU"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_gradient_function = grad(binary_loss_function, argnums=1, allow_int=True)"
      ],
      "metadata": {
        "id": "SdJgvf9Eg-iY"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_gradient_function(np.array([0,1]))"
      ],
      "metadata": {
        "id": "mefXzAxhhOhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}