{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IFibla/Aprenentatge-Automatic/blob/master/Problema%201/Individual-%C2%BFKullback-que%3F/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 쯂ullback que?\n",
        "\n",
        "Cuando trabajamos con modelos que representan una distribuci칩n de probabilidad nuestro objetivo es hacer que la distribuci칩n de los datos se acerque lo m치s posible a las probabilidades que nos da el modelo sobre esos datos. Existen muchas maneras de calcular esa diferencia, una com칰n es usar funciones de divergencia, entre ellas la divergencia de Kullback-Leibler es la m치s usada. Dadas dos distribuciones de probabilidad 洧녞 y 洧녟 se define asumiendo que sean distribuciones discretas como:\n",
        "\n",
        "$$\n",
        "KL(P|Q)=\\sum_{i=1}^{N}P(i)\\cdot \\log(\\frac{P(i)}{Q(i)})\n",
        "$$\n",
        "\n",
        "En el caso de distribuciones continuas, simplemente substituimos el sumatorio por una integral.\n",
        "\n"
      ],
      "metadata": {
        "id": "tNlYxswldlOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJTYZijMdg_D"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad, jit\n",
        "from sklearn.datasets import make_classification, make_circles\n",
        "import jax.numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "9f7kVGAJf12-"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 1"
      ],
      "metadata": {
        "id": "tv_mwq9mg7Iu"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "HJexNey3i2Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_make_classification_list(in_list):\n",
        "  out_list = []\n",
        "  for i in range(len(in_list[0])):\n",
        "    out_list.append([l[i] for l in in_list])\n",
        "  return out_list"
      ],
      "metadata": {
        "id": "i3VcSnL1NrAB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_matplotlib_histogram(X, labelX, labelY, title, rangeX=None, rangeY=None, bins=30):\n",
        "  if rangeX is None: \n",
        "    rangeX = [np.min(X), np.max(X)]\n",
        "  plt.hist(X, bins=bins, alpha=1, range=rangeX, density=True, stacked=True)  \n",
        "  plt.xlabel(labelX)\n",
        "  plt.ylabel(labelY)\n",
        "  plt.title(title)\n",
        "  if rangeY is not None:\n",
        "    plt.ylim((rangeY[0], rangeY[1]))\n",
        "  plt.legend([len(x) for x in X] if len(X) > 1 else \"X\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gn2wU130i6E-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Siendo 洧녦 una muestra de datos 洧논1, ... , 洧논洧녵 de valores discretos, donde podemos estimar su distribuci칩n 洧녞 a partir de su frecuencia y 洧녟 es una distribuci칩n de probabilidad sobre el mismo rango de valores discretos. Demuestra que optimizar 洧 洧(洧녞 |洧녟) es equivalente a optimizar la log verosimilitud negativa de 洧녟 sobre los datos."
      ],
      "metadata": {
        "id": "hWDNwM-oQZiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_minus_log_verosimilitud_q = lambda P: sum(1/np.log(P)) - (len(P) * np.log(len(P)))\n",
        "a_diff_minus_log_verosimilitud_q = lambda P: sum(1/np.log(P))\n",
        "\n",
        "a_optimise_kl = lambda P: (len(P) + len(P) * np.log(len(P))) * sum(P)\n",
        "a_diff_optimise_kl = lambda P: (len(P) + len(P) * np.log(len(P))) * sum(P)"
      ],
      "metadata": {
        "id": "620AnuQNQg10"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Todo modelo de clasificaci칩n es una distribuci칩n de probabilidad sobre un conjunto de valores discretos, por lo que podemos ajustar un modelo probabil칤stico para clasificaci칩n haciendo que las probabilidades que obtenga para una muestra se ajusten a las de los datos. Usa la funci칩n make_classification de scikit-learn para crear un conjunto de datos de clasificaci칩n de dos dimensiones y 100 ejemplos. Tendr치s dar un valor 0 al par치metro n_redundant y un valor 1 al par치metro n_clusters_per_class. Da un valor tambi칠n al par치metro random_state para que los experimentos sean reproducibles. El problema que generar치 ser치 de clasificaci칩n binaria."
      ],
      "metadata": {
        "id": "ABudX8lbfskY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)\n"
      ],
      "metadata": {
        "id": "1xg3L-mJgVOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b_sample = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=RANDOM_STATE)\n",
        "b_sample_prob = np.array(resize_make_classification_list(b_sample[0]))\n",
        "b_sample_label = np.array(b_sample[1])\n",
        "b_sample_prob, b_sample_label"
      ],
      "metadata": {
        "id": "ThekmQiTgRZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Podemos crear un modelo probabil칤stico con una funci칩n linear $f(w,x)=w\\cdot x$. Para obtener probabilidades simplemente tenemos que aplicar sobre el resultado una funci칩n que de un valor entre 0 y 1. Por ejemplo la funci칩n sigmoide 洧랥:\n",
        ">\n",
        "> $픢(x)=\\frac{1}{1+e^x}$\n",
        ">\n",
        "> A partir de la divergencia de Kullback-Leibler simplificando para problemas binarios podemos llegar a la funci칩n de p칠rdida de entrop칤a cruzada binaria (binary cross entropy):\n",
        ">\n",
        "> $BCE(p(x), y )=y \\cdot \\log(p(x)) + (1-y) \\cdot \\log(1-p(x))$\n",
        ">\n",
        "> Donde 洧녷(洧논) es la probabilidad que le asigna el modelo a un ejemplo, e 洧녽 es la etiqueta que le corresponde a los datos. Implementa un algoritmo de descenso de gradiente usando JAX con la funci칩n de entrop칤a cruzada binaria. Explora diferentes tasas de aprendizaje. Comenta lo que observes en el comportamiento del error y los par치metros durante la optimizaci칩n. Escoge un n칰mero de iteraciones y un valor para decidir el final de la optimizaci칩n que te parezcan adecuados."
      ],
      "metadata": {
        "id": "CmlvJqMkQYJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. jax.grad](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html)"
      ],
      "metadata": {
        "id": "z_lBNg2QVmte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_sigmoid_function = lambda x: 1 / (1 + np.exp(x))\n",
        "cross_binary_entropy = lambda Px, y: np.array(y * np.log(Px) + ((1 - y) * np.log(1-Px))).mean()"
      ],
      "metadata": {
        "id": "4Y-JlCQ1dJgS"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_gradient_function = grad(cross_binary_entropy, argnums=1, allow_int=True)"
      ],
      "metadata": {
        "id": "RzBjZYtBVEM6"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_sample_prob, c_sample_label = c_sigmoid_function(b_sample_prob), b_sample_prob"
      ],
      "metadata": {
        "id": "DZDxEALGb-8F"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_gradient_function(c_sample_prob, c_sample_label)"
      ],
      "metadata": {
        "id": "kzHIt3I0ejPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Genera un conjunto de datos con la funci칩n make_circles de scikit-learn usando el valor 0,1 para el par치metro noise. Optimiza el modelo para varios par치metros iniciales diferentes del modelo. Cuenta que esta que sucediendo e intenta explicar el porqu칠."
      ],
      "metadata": {
        "id": "ua1BColsfU3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Documentation. sklearn.datasets.make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)"
      ],
      "metadata": {
        "id": "j3Ah5WYpfbID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_sample = make_circles(n_samples=100, noise=0.1, random_state=RANDOM_STATE)\n",
        "d_sample"
      ],
      "metadata": {
        "id": "EhwJHwIzfYO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> La funci칩n de entrop칤a cruzada parece una funci칩n extra침a para optimizar cuando lo que nos interesa es un modelo que tenga el m칤nimo n칰mero de ejemplos mal clasificados. En este caso se corresponder칤a a la funci칩n de p칠rdida 0/1, que en el caso de probabilidades asignar칤a una p칠rdida de 0 a valores menores que 0.5 y 1 en caso contrario 쯇orqu칠 no es una buena idea optimizar directamente esta funci칩n? Representa las dos funciones."
      ],
      "metadata": {
        "id": "3OvLeqrTgYYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_loss_function = lambda Px: 0 if Px < 0.5 else 1"
      ],
      "metadata": {
        "id": "Y8gwO9EWgeFU"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_gradient_function = grad(binary_loss_function, argnums=1, allow_int=True)"
      ],
      "metadata": {
        "id": "SdJgvf9Eg-iY"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e_gradient_function(np.array([0,1]))"
      ],
      "metadata": {
        "id": "mefXzAxhhOhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}